# 하둡 설정 #

- 물리 Node 준비
- 배포

## Node 준비 ##

- name node
	- host name : name01
	- IP : 192.168.191.128
- secondary node
	- host name : name02
	- IP : 192.168.191.129
- data node 1
	- host name : data01
	- IP : 192.168.191.130
- data node 2
	- host name : data02
	- IP : 192.168.191.131
- data node 3
	- host name : data03
	- IP : 192.168.191.132


### hostname 및  IP 설정 ###

각각의 노드에서 수행

	[root@hostname ~]# vi /etc/sysconfig/network

	    NETWORKING=yes
    	HOSTNAME=name01

    [root@hostname ~]# vi /etc/sysconf/network-scripts/ifcfg-eth0

    	DEVICE=eth0
    	TYPE=Ethernet
    	UUID=26f57d4d-3afa-43ae-b968-d8191d76fd4f
    	ONBOOT=yes
    	NM_CONTROLLED=yes
    	BOOTPROTO=static
    	NETWORK=192.168.191.0
    	NETMASK=255.255.255.0
    	IPADDR=192.168.191.128
    	GATEWAY=192.168.191.2
    	HWADDR=00:0C:29:CA:39:83
    	DEFROUTE=yes
    	PEERDNS=yes
    	PEERROUTES=yes
    	IPV4_FAILURE_FATAL=yes
    	IPV6INIT=no
    	NAME="System eth0"

### hadoop 계정 생성 ###
모든 노드에서 동일하게 수행

	[root@hostname ~]# groupadd hadoop
    [root@hostname ~]# adduser -g hadoop hadoop
	[root@hostname ~]# passwd hadoop

### ssh key 설정 및 배포 ###

	[root@name01 ~]# su -l hadoop

	# 키 생성
	[hadoop@name01 ~]$ ssh-keygen -t rsa
	Generating public/private rsa key pair.
	Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):
	Created directory '/home/hadoop/.ssh'.
	Enter passphrase (empty for no passphrase):
	Enter same passphrase again:
	Your identification has been saved in /home/hadoop/.ssh/id_rsa.
	Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
	The key fingerprint is:
	37:0e:7a:c2:6d:97:67:70:c9:db:8d:eb:b0:b6:33:b6 hadoop@name01
	The key's randomart image is:
	+--[ RSA 2048]----+
	|                 |
	|                 |
	|                 |
	|           . .   |
	|        S + +    |
	|     . o + = o o |
	|      + + + = o .|
	|       + . o=o . |
	|           oE*o  |
	+-----------------+

	# 키 배포
	# 개별 노드에 모두 수행 ( name01, name02, data01, data02, data03 )
	[hadoop@name01 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@name01
	The authenticity of host 'name01 (192.168.191.128)' can't be established.
	RSA key fingerprint is 41:56:ab:f1:0e:11:77:ad:12:2e:91:14:39:79:b8:99.
	Are you sure you want to continue connecting (yes/no)? yes
	Warning: Permanently added 'name01,192.168.191.128' (RSA) to the list of known hosts.
	hadoop@name01's password:
	Now try logging into the machine, with "ssh 'hadoop@name01'", and check in:
	
	  .ssh/authorized_keys
	
	to make sure we haven't added extra keys that you weren't expecting.
    
    [hadoop@name01 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@name02
    [hadoop@name01 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@data01
    [hadoop@name01 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@data02
    [hadoop@name01 ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@data03

### 개별 노드에서 hadoop file 옮기기 ###
	
	[hadoop@hostname ~]$ cd /opt
	[hadoop@hostname ~]$ cp -R /usr/local/src/hadoop-2.5.2 ./
	[hadoop@hostname ~]$ ln -s hadoop-2.5.2 hadoop
	[hadoop@hostname ~]$ chown -R hadoop:hadoop hadoop-2.5.2 hadoop

### hadoop 설정 파일 수정 - hadoop-env.sh ###

	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 hadoop]$ vi hadoop-env.sh

		#export JAVA_HOME=${JAVA_HOME}
		export JAVA_HOME=/usr/local/java

### hadoop 설정 파일 수정 - masters ###

	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 hadoop]$ vi masters

		name01

### hadoop 설정 파일 수정 - slaves ###

	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 hadoop]$ vi slaves
	
		data01
		data02
		data03

### hadoop 설정 파일 수정 - core-site.xml ###

	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 hadoop]$ vi core-site.xml

		<configuration>
		  <property>
		    <name>fs.defaultFS</name>
		    <value>hdfs://name01:9010</value>
		  </property>
		</configuration>

### hadoop 설정 파일 수정 - hdfs-site.xml ###
> name01 과 name02 의 hadoop 홈디렉토리에서..<br>
> /home/hadoop/data/dfs/namenode<br>
> /home/hadoop/data/dfs/namesecondary<br>
> 를 미리 만들어 둔다.

	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 hadoop]$ vi hdfs-site.xml

		<configuration>
			<property>
		        <name>dfs.replication</name>
		        <value>3</value>
		    </property>
		
		    <property>
		        <name>dfs.namenode.name.dir</name>
		        <value>file:///home/hadoop/data/dfs/namenode</value>
		    </property>
		
		    <property>
		        <name>dfs.namenode.checkpoint.dir</name>
		        <value>file:///home/hadoop/data/dfs/namesecondary</value>
		    </property>
		
		    <property>
		        <name>dfs.datanode.data.dir</name>
		        <value>file:///data1</value>
		    </property>
		
		    <property>
		        <name>dfs.http.address</name>
		        <value>name01:50070</value>
		    </property>
		
		    <property>
		        <name>dfs.secondary.http.address</name>
		        <value>name01:50090</value>
		    </property>
		</configuration>

### hadoop 설정 파일 수정 - mapred-site.xml ###

	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 hadoop]$ cp mapred-site.xml.template mapred-site.xml

		<configuration>
		    <property>
		        <name>mapreduce.framework.name</name>
		        <value>yarn</value>
		    </property>
		</configuration>		


### hadoop 설정 파일 수정 - yarn-site.xml ###
> name01 과 name02 의 hadoop 홈디렉토리에서..<br>
> /home/hadoop/data/yarn/nm-local-dir<br>
> /home/hadoop/data/yarn/system/rmstore<br>
> 를 미리 만들어 둔다.

	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 hadoop]$ vi yarn-site.xml

		<configuration>
			<property>
				<name>yarn.nodemanager.aux-services</name>
				<value>mapreduce_shuffle</value>
			</property>
		
			<property>
				<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
				<value>org.apache.hadoop.mapred.ShuffleHandler</value>
			</property>
		
			<property>
				<name>yarn.nodemanager.local-dir</name>
				<value>/home/hadoop/data/yarn/nm-local-dir</value>
			</property>
		
			<property>
				<name>yarn.resourcemanager.fs.state-store.uri</name>
				<value>/home/hadoop/data/yarn/system/rmstore</value>
			</property>
		
			<property>
				<name>yarn.resourcemanager.hostname</name>
				<value>name01</value>
			</property>
		</configuration>

### hadoop 배포 ###
	
	[hadoop@name01 ~]$ cd /opt/hadoop/etc/hadoop
	[hadoop@name01 ~]$ scp * name02:/opt/hadoop/etc/hadoop
	[hadoop@name01 ~]$ scp * data01:/opt/hadoop/etc/hadoop
	[hadoop@name01 ~]$ scp * data02:/opt/hadoop/etc/hadoop
	[hadoop@name01 ~]$ scp * data03:/opt/hadoop/etc/hadoop

### 개별 노드에서 iptables 정지 ###
	
	[root@hostname ~]$ service iptables save
	[root@hostname ~]$ service iptables stop
	[root@hostname ~]$ chkconfig iptables off

### 개별 노드의 /etc/hosts 파일 내용 확인 ###

	#
	# name01
	#

	127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
	::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
	
	192.168.191.128 name01
	192.168.191.129 name02
	192.168.191.130 data01
	192.168.191.131 data02
	192.168.191.132 data03

	#
	# name02
	#
	
	127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
	::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
	
	192.168.191.128 name01
	192.168.191.129 name02
	192.168.191.130 data01
	192.168.191.131 data02
	192.168.191.132 data03

	#
	# data01
	#
	
	127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
	::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
	
	192.168.191.128 name01
	192.168.191.129 name02
	192.168.191.130 data01
	192.168.191.131 data02
	192.168.191.132 data03
	
	#
	# data02
	#

	127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
	::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
	
	192.168.191.128 name01
	192.168.191.129 name02
	192.168.191.130 data01
	192.168.191.131 data02
	192.168.191.132 data03

	#
	# data03
	#
	
	127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
	::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
	
	192.168.191.128 name01
	192.168.191.129 name02
	192.168.191.130 data01
	192.168.191.131 data02
	192.168.191.132 data03

### hdfs 포맷 ###

	[hadoop@name01 ~]$ cd /opt/hadoop
	[hadoop@name01 ~]$ ./bin/hdfs namenode -format
	
### hadoop 데몬 실행 ###

	[hadoop@name01 ~]$ cd /opt/hadoop
	[hadoop@name01 ~]$ ./sbin/start-all.sh